#A Comparison of Three Prominent Journal Metrics with Expert Judgement of Journal Quality
##Included in the [Proceedings](http://sti2014.cwts.nl/News?article=n-v2&title=Proceedings+online) of the [2014 STI conference](http://sti2014.cwts.nl/Home)

##Before you read

###Who is the author?

Peter Haddawy – Faculty of ICT, Mahidol University, Phuttamonthon 4 Road, Nakhon Pathom, 73170 (Thailand)

Saeed-Ul Hassan – Scientometrics Lab, Information Technology University - Punjab, Ferozepur Road, Lahore, 54770 (Pakistan)

###What’s the article about?

Compares three journal impact metrics—JIF, SNIP and SJR—against the journal rankings by experts from a government study, done as part of the “Excellence in Research for Australia” initiative.  (See the webpage: [*Tiers for the Australian Ranking of Journals*](http://www.arc.gov.au/era/tiers_ranking.htm).)

----
##First time you read

###What are the main claims?

All three metrics correlate strongly with the rankings; SNIP correlates the best.

###What are the conclusions?

The strongest correlation by SNIP is due to judging “quality” of journals as opposed to their “popularity”.

###What questions do you have about the reading?

How were the ERA rankings assembled?  (Averages? Medians? Modes? etcetcetc)

----
##Second time you read

###What’s the evidence for each of those claims?

###Why was this reading assigned?

###Answer your own questions if you can

----
##Third time you read

###What’s missing?

###What are the weaknesses in those claims and evidence?

###What can you do with what you learned from this reading?

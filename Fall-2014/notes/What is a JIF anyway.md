#What exactly is a Journal Impact Factor, anyway?

The Journal Impact Factor is a proprietary ranking of scientific journals, assessed and published by one of the divisions of [Thomson Reuters](http://thomsonreuters.com/journal-citation-reports/).  It is based on citations between the journals indexed by Thomson Reuters' [Web of Science](http://wokinfo.com/citationconnection/) content management system.

The method used by the JIF to determine a journal's “impact factor” has been traced back to {Gross, P. L. K., Gross, E. M. (1927), “College libraries and chemical education”. *Science*, 66 (1713) : 385–389}.  That work was intended to aid libraries at research institutions in deciding what journals would most benefit the researchers they supported  (Archambault & Larivière, 2009).

The basic method for calculating an impact factor is to take the number of citations to articles in a given journal over some time frame (2 years is the most common time frame used) and divide that by the total number of citable articles published in that journal over the same time frame.  In effect, you're calculating the average number of citations per article.

(*Big red flag right there.  These days you should never trust any process that uses an average without justifying the hell out of it.*)

There are any number of criticisms for how the JIF is calculated.  Archambault & Larivière (2009) have five main criticisms:

1. The JIF is optimized in a US-centric manner (English-language journals are preferred, etc.).
2. The JIF is misused to infer the “impact” of individual articles in journals
3. Including journal self-citations in the calculation provides an easy way for editors to manipulate the impact factor of their journals.
4. The items that are included in the “number of citations” and the “citable articles” are different (e.g., citations to a book review are included in the count of citations, but book reviews are not counted as citable items).
5. The initial use of a two-year time window disadvantaged certain fields, and certain types of articles.  (There are now 5-year JIFs, but that is still an essentially arbitrary window.)

Bornmann, et al. (2011) cover many of those same issues, and also throw in:

* Different “citing behaviors”, number of journals, and number of participants in different fields make comparisons of JIFs between fields unjustifiable, yet that happens (to justify funding, promotions, etc.).
* Multiple-language versions of the same journal have different JIFs.  Citations that include multiple-language versions of the same article inflate the JIF calculations.

There are now a number of competing indicies.  For example, Haddawy & Hassan (2014) compared the Journal Impact Factor, the Source Normalized Impact per Paper (SNIP) and the SCImago Journal Rank (SJR) against a non-numeric ranking by experts based on the perceived quality of the papers in each journal.  They briefly discuss the relative merits of each of the indicies.

----

In the conclusion of their article, Archambault & Larivière (2009) write:

>At this juncture, the best course of action therefore seems to be redesigning the tool from the ground up. We could deal knowingly with inter-field variations, journal self- citations, citable and non-citable items, citation windows – all technical aspects for which several teams have already suggested remediation and potentially robust solutions. …

That is followed by:

>But the most important question we should consider is ‘who are “we”’?

Which struck me as an “Aha!” moment, until I read the next sentence:

>At present, the bibliometric and scientific communities appear to be excluded from this “we”, as they have so far been largely unable to effect significant change. The evolution of the dominant indicator and the provision of alternate, widely available solutions have been placed into the hands of a private firm that has failed to respond to increasingly numerous calls for action.

I was hoping for some kind of inclusion of practitioners, but Archambault & Larivière seem to be recommending that the determination of “impact” of research be pulled even deeper into the research community, and kept behind the walls of academia.  But I may be reading too much into that.

Anyway, I want to take a step back for a moment:

Archambault & Larivière shouldn't be surprised that “the bibliometric and scientific communities appear to be excluded from this ‘we’, as they have so far been largely unable to effect significant change”.  Why fix something that isn't broke?

The continued popularity of the JIF and similar indicies is part-and-parcel with a lack of common will to change it.  People like that it's simple.  People like that it's quantified.  It's so very easy and efficient to determine that “2 > 1”.  What's even more lovable about JIF is that bigger is better.  So 3 beats 2 beats 1.  No thinking or judgement required.

I see this as falling under my “tyranny of numbers” diatribe.  The relative *value* of journals has been reduced to a simple quantity.  And we will then use that to quantify the value of not only articles, but the people who wrote the articles.  Is that a valid use?  Who cares—as long as it's easy!  Besides, it *must* be right, because 2 > 1!

Indeed, the complexity and opacity of the process to determine JIFs actually works to it's favor.  You can't argue against it's validity if you don't really understand it.  More importantly, since it does give you a nice, simple number to use, it's easy to convince yourself that you “understand it well enough” to use it.

But in the end, it's like using your weight to judge your state of health or your attractiveness.  It might be strongly correlated with the things you care about, but it's way too easy to start believing that it's the key to controlling the things you care about.  When in fact the cause-and-effect often are the other way around.

In the case of JIF: your article isn't more important than other articles because it's in a journal with a high JIF.  That journal has a high JIF because it publishes more important articles than it does trivial ones.  But your article may well be one of the trivial ones.

----

Up next:

* Priem, J. 2010. *The alt-metrics manifesto*. Available at [altmetrics.org/manifesto](http://altmetrics.org/manifesto/).

And maybe I can dig up some other things that criticize the incestuous nature of JIF, etc.

----
----

###References

  * Archambault, Éric, and Vincent Larivière. 2009. “History of the journal impact factor: Contingencies and consequences.” *Scientometrics* 79, no. 3 : 635-649.
  * Bornmann, Lutz, Werner Marx, Armen Yuri Gasparyan, and George D. Kitas. 2012. “Diversity, value and limitations of the journal impact factor and alternative metrics.” *Rheumatology international* 32, no. 7 : 1861-1867.
  * Haddawy, Peter, and Hassan, Saeed-Ul. 2014. “A comparison of three prominent journal metrics with expert judgement of journal quality.” In: Noyons, Ed (Ed.): *Context counts: pathways to master big and little data. Proceedings of the 19th International Conference on Science and Technology Indicators 2014 Leiden*. p. 238-240. DOI: 10.13140/2.1.3086.5285. ISBN 978-90-817527-1-8



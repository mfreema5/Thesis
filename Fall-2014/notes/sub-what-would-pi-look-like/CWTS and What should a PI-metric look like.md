#CWTS and practical impact

On the CWTS website I didn't find any tools or reports or the like that covers “practical impact” as I've been envisioning it so far.  The apparently have a ‘commercial divison’ of sorts, CWTS B.V., that will provide research statistics to firms.  Though I get the impression that it does more training/education than providing of analysis reports or data packages.  (See: [CWTS webpages](Fall-2014/notes/CWTS-webpages.md).)

But while going over the papers available on the site, I finally got around to reading through “Who Reads Research Articles? An Altmetrics Analysis of Mendeley User Categories” (Mohammadi *et al.*, 2014).  And it would seem to be a gold mine of references in the fields of bibliometrics and scientometrics, which are relevant to my interests.  So, I'm thinking that I might need to go back and do another iteration through the existant literature.

Among other things of possible interest, there appears to be research (as yet undiscovered by me) on the differences between “reading” and “citing” activities, and why using only citation metrics can therefore misrepresent the over-all impact of a paper.  “Pure readers” will use papers for practical application, or teaching&mdash;which should probably count as some sort of “practical impact”, since the students who are taught some piece of research are much more likely to employ it later than they are research that languishes in a journal somewhere.

Up until now I had been mentally stuck in thinking about altmetrics as being just another form of citations, but ones that are not in academic journals.  The Mohammadi *et al.* (2014) paper goes a completely different route, looking at readership data from Mendeley.  That is not to say that theirs is a novel approach&mdash;they reference a number of other papers that do similar analyses, using various “webometrics” applied to various websites or content-management packages.  I'd always had a feeling that there was a body of research relevant to my thesis lurking out there in the infometrics field, but I didn't have any good way of getting to it.  This paper might be the entrance I need.

So, anyway, back to the point at hand: it doesn't seem like CWTS is already doing what I'm proposing.  Though it's definitely related to many of their research goals.

#What should a ‘practical impact’ metric look like?

***If you don't reward a behavior, people won't engage in it.  And if you don't properly measure a behavior, you can't properly reward it.***

What should a “practical impact” metric look like?  I don't think it's feasible to have just one.  Not only are there a number of different paths to identifying either practical impact citation-like events, readership data, or teaching-related data, etc., but there may be no way to make quantitative comparisons between different practical impacts.  Much like the effect of “citation cultures” on citation metrics, not only will practical impacts from different paths be largely incomparable, but the context of the impact will make a large difference in its “societal value” (for lack of a better term).

Narrowing the focus of my thesis down to one sort, in one field might make it possible to generate something as an example of the concept.  Though, I don't know if I need decide on what the example should be right now.  More importantly, I'm still more interested in—or, rather, I think theres's more value to be had from—exploring and defining the concept of measuring practical impact.

Meaning, I think it will be more useful to flesh out the notion that practical impact needs to be considered separately from other sorts of impact.  I still find it odd that altmetrics are being “validated” by examining their correlation to existing metrics.

Is practical impact an extension of other metrics, or an adjunct?  Where others want to show correlations between existing metrics and altmetrics, I want to demonstrate that practical impact is an independent quality that is *not* being properly captured in citation metrics, and is therefore being ignored.

So maybe: pick some field, pick some path to measuring practical impact, generate the numbers for some class of papers.  Then compare them to existing metrics.  How?  What do I expect to see?  Zero correlation would be odd—how can something have practical impact but no citation-based impact?  Perfect correlation would make me wonder if I was really measuring something independent of citations.  Maybe I need to go refresh my spotty memory of statistical theory.


----

##Reference

Mohammadi, Ehsan, Mike Thelwall, Stefanie Haustein, and Vincent Larivière. 2014. “Who Reads Research Articles? An Altmetrics Analysis of Mendeley User Categories.” *Journal of the Association for Information Science and Technology*, in press.

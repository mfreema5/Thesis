#Inclusion of ‘practical impact’ in current metrics

Are the potential indicators of practical impact already being included in existing “impact” metrics?

##Bibliometrics and scientometrics and informetrics

The question of practical impact overlap in current metrics becomes easier to approach if you first consider the following definitions of the metrics-related fields. (They are based on the historical survey given by [Hood & Wilson (2001)](http://doi.org/10.1023/A:1017919924342) – see also the local file [“Bibliometrics vs. Scientometrics vs. Informetrics.md”](/Fall-2014/notes/Bibliometrics%20vs.%20Scientometrics%20vs.%20Informetrics.md).)

* *Bibliometrics*
  * The quantitative study of scholarly articles and/or books, or electronic equivalents (i.e., the typical material in an academic “bibliography”).
* *Scientometrics*
  * The quantitative study of science as an activity. The journal [*Scientometrics*](http://link.springer.com/journal/11192) describes the field as covering “the quantitative features and characteristics of science and scientific research”, and specifically “the development and mechanism of science.” Scientometrics can have application in making science policy.
    * Scientometrics overlaps with those bibliometrics that are applied to the field of science.
* *Informetrics*
  * Most any definition of informetrics would subsume both bibliometrics and scientometrics, since it is the quantitative study of science communication.  But, since the other two fields are established, “informetrics” is typically only used to refer to analyses which fall outside of bibliometrics and scientometrics.
    * More interestingly, [Ingwersen & Christensen (1997)](http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199703%2948:3%3C205::AID-ASI3%3E3.0.CO;2-0/abstract) defined informetrics as: “…a recent extension of the traditional bibliometric analyses also to cover non-scholarly communities in which information is produced, communicated, and used.”

So, the fields of *bibliometrics* and *scientometrics* are restricted to “scholarly” information.  And it seems that most of the metrics I've looked at so far follow that same sort of restriction, which I referred to as being “silo'd” in the following table:

| index / indicator | article-level | silo'd | mono-db |
|-------------------|---------------|--------|---------|
| JIF               | N             |  Y     |  Y      |
| II-WoS            | N             |  Y     |  Y      |
| JFIS              | N             |  Y     |  Y      |
| SNIP              | N             |  Y     |  Y      |
| RIP               | N             |  Y     |  Y      |
| SJR               | N             |  Y     |  Y      |
| *h*-Index         | Y             |  Y     |  N      |
| **P**             | Y             |  Y     |  N      |
| **C**             | Y             |  Y     |  N      |
| **CPP**           | Y             |  Y     |  N      |
| **Pnc**           | Y             |  Y     |  N      |
| **% SC**          | Y             |  Y     |  N      |
| **CPP/JCSm**      | Y             |  Y     |  N      |
| **CPP/FCSm**      | Y             |  Y     |  N      |
| **JCSm/FCSm**     | Y             |  Y     |  N      |

(For explanation of the TLAs and more tabled information, see: [“Mapping the Metrics.md”](/Fall-2014/notes/Mapping%20the%20Metrics.md).)

But evidence of ‘practical impact’ will be *outside* of “scholarly” data sources, by definition.  So, bibliometric and scientometric metrics are never going to overlap with a search for practical impact.

##Altmetrics

The only existing metrics within informetrics that are likely to include practical impact are usually called “altmetrics”

>Altmetrics is the study of the mentions (i.e. likes, shares, comments, tweets, blog posts, bookmarks, saves, recommendations, etc.) of scientific outputs in social web tools such as Facebook, Twitter, Wikipedia, blogs, news media, and reference management tools. (**CWTS**)

###CWTS

The Centre for Science and Technology Studies ([CWTS](http://www.cwts.nl/Home)) at Leiden University, in Leiden, the Netherlands, has a research group that studies science's “socio-economic and cultural effects… on society at large”.  The ‘Society Using Research’ ([SURe](http://www.cwts.nl/Societal-Impact-of-Research)) group has a “research line in [Altmetrics](http://www.cwts.nl/Altmetrics)”.

As far as I can tell, they haven't released any actual metrics, but they have been publishing articles in the area, making presentations at conferences, etc.  For more information and a list of publications, see [“CWTS-nl.md”](/Fall-2014/notes/CWTS-nl.md), and for a summary of one CWTS-related presentation, see [“Overview of bibliometric analysis.md”](/Fall-2014/notes/Overview%20of%20bibliometric%20analysis.md).

###Altmetric LLP

I still don't really know exactly what metrics [Altmetric LLP](http://www.altmetric.com/about.php) provides.  Presumably there would be overlap,  but their metrics are mostly commercial and closed source, so I'm not sure: A – how much I can learn, and B – how much it would matter if it does overlap.

The other issue, of course, is for what purposes does Altmetric LLP intend their metrics?  What reason the give for their product is: “Increasingly [researchers] need to show the impact of their papers, books and datasets are having beyond just citations.”  Show to whom?  For what end?

Similarly, CWTS-SURe states that (some of) their goals are:

* “To develop an independent method to retrieve data that analyzes and/or quantifies scientific outputs (not only publications) describing societal quality of research.…”

* “To validate, value, and visualize the results of evaluation of societal quality. In other words, what is the meaning (value) of these societal outputs.”

How would practical impact tie in with their analysis of the “societal quality” of research?

###Required or rewarded

Also, I don't see either Altmetric LLP or CWTS-SURe considering the issue of whether a measure “altmetric value” is going to used to judge criteria that are required or criteria that are rewarded.  They seem to be simply offering them up as supplements to the existent metrics, which are being used as surrogates for quantifying various required properties of research.

Of course, if existent metrics are misused, should providers of altmetrics worry about how they will be used?  Which brings us to…

##‘Leiden Manifesto’

CWTS runs a conference about Science and Technology Indicators.  The 19^th annual STI conference&mdash;“Context Counts: Pathways to Master Big and Little Data”&mdash;was convened in Leiden, just last month (September 2014).

As reported on a CWTS-related blog, “[The Citation Culture](http://wp.me/p1RSZG-8d)”, during a panel session [Diana Hicks](http://www.iac.gatech.edu/faculty-and-staff/faculty/bio/hicks), a professor and department chair in the School of Public Policy at the Georgia Tech, proposed a set of guidelines for the use of quantitative metrics in research assessment, which some are calling the ‘Leiden Manifesto’.

The best twitter-sized summary of that proposal is perhaps: “Metrics properly used support assessments; they do not substitute for judgment.”  (For more details, see: [“Leiden Manifesto.md”](/Fall-2014/notes/Leiden%20Manifesto.md).)

I'm wondering if the required/rewarded distinction is one that's never been consider, or has already been considered and dismissed.

##Overlap?  Not entirely, if at all.

Either way, neither of the potential sources of altmetrics seem to have given any specific recommendations of the use of altmetrics.  So, even if there is overlap in the inclusion of practical impact data, there likely won't be any overlap in how that data is treated/presented.




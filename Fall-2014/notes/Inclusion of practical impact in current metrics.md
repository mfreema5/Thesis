#Inclusion of ‘practical impact’ in current metrics

Are the potential indicators of practical impact already being included in existing “impact” metrics?

##Bibliometrics and scientometrics and informetrics

The question of practical impact overlap in current metrics becomes easier to approach if you first consider the following definitions of the metrics-related fields. (They are based on the historical survey given by [Hood & Wilson (2001)](http://doi.org/10.1023/A:1017919924342) – see also the local file [“Bibliometrics vs. Scientometrics vs. Informetrics.md”](/Fall-2014/notes/Bibliometrics%20vs.%20Scientometrics%20vs.%20Informetrics.md).)

* *Bibliometrics*
  * The quantitative study of scholarly articles and/or books, or electronic equivalents (i.e., the typical material in an academic “bibliography”).
* *Scientometrics*
  * The quantitative study of science as an activity. The journal [*Scientometrics*](http://link.springer.com/journal/11192) describes the field as covering “the quantitative features and characteristics of science and scientific research”, and specifically “the development and mechanism of science.” Scientometrics can have application in making science policy.
    * Scientometrics overlaps with those bibliometrics that are applied to the field of science.
* *Informetrics*
  * Most any definition of informetrics would subsume both bibliometrics and scientometrics, since it is the quantitative study of science communication.  But, since the other two fields are established, “informetrics” is typically only used to refer to analyses which fall outside of bibliometrics and scientometrics.
    * More interestingly, [Ingwersen & Christensen (1997)](http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199703%2948:3%3C205::AID-ASI3%3E3.0.CO;2-0/abstract) defined informetrics as: “…a recent extension of the traditional bibliometric analyses also to cover non-scholarly communities in which information is produced, communicated, and used.”

So, the fields of *bibliometrics* and *scientometrics* are restricted to “scholarly” information.  And it seems that most of the metrics I've looked at so far follow that same sort of restriction, which I referred to as being “silo'd” in the following table:

| index / indicator | article-level | silo'd | mono-db |
|-------------------|---------------|--------|---------|
| JIF               | N             |  Y     |  Y      |
| II-WoS            | N             |  Y     |  Y      |
| JFIS              | N             |  Y     |  Y      |
| SNIP              | N             |  Y     |  Y      |
| RIP               | N             |  Y     |  Y      |
| SJR               | N             |  Y     |  Y      |
| *h*-Index         | Y             |  Y     |  N      |
| **P**             | Y             |  Y     |  N      |
| **C**             | Y             |  Y     |  N      |
| **CPP**           | Y             |  Y     |  N      |
| **Pnc**           | Y             |  Y     |  N      |
| **% SC**          | Y             |  Y     |  N      |
| **CPP/JCSm**      | Y             |  Y     |  N      |
| **CPP/FCSm**      | Y             |  Y     |  N      |
| **JCSm/FCSm**     | Y             |  Y     |  N      |

(For explanation of the TLAs and more tabled information, see: [“Mapping the Metrics.md”](/Fall-2014/notes/Mapping%20the%20Metrics.md).)

But evidence of ‘practical impact’ will be *outside* of “scholarly” data sources, by definition.  So, bibliometric and scientometric metrics are never going to overlap with a search for practical impact.

##Altmetrics

The only existing metrics within informetrics that might already include practical impact are those that are usually called “altmetrics”

>Altmetrics is the study of the mentions (i.e. likes, shares, comments, tweets, blog posts, bookmarks, saves, recommendations, etc.) of scientific outputs in social web tools such as Facebook, Twitter, Wikipedia, blogs, news media, and reference management tools. ([CWTS](http://www.cwts.nl/Home))

###CWTS

The Centre for Science and Technology Studies ([CWTS](http://www.cwts.nl/Home)) at Leiden University, in Leiden, the Netherlands, has a research group that studies science's “socio-economic and cultural effects… on society at large”.  The ‘Society Using Research’ ([SURe](http://www.cwts.nl/Societal-Impact-of-Research)) group has a “research line in [Altmetrics](http://www.cwts.nl/Altmetrics)”.

As far as I can tell, they haven't released any actual metrics, but they have been publishing articles in the area, making presentations at conferences, etc.  For more information and a list of publications, see [“CWTS-nl.md”](/Fall-2014/notes/CWTS-nl.md), and for a summary of one CWTS-related presentation, see [“Overview of bibliometric analysis.md”](/Fall-2014/notes/Overview%20of%20bibliometric%20analysis.md).

###Altmetric LLP

I still haven't dug in to what exactly [Altmetric LLP](http://www.altmetric.com/about.php) provides.  Presumably there would be overlap.  But it's a commericial, closed source metric, so I'm not sure, A – how much I can learn, and B – how much it would matter if it does overlap.

The other issue, of course, is for what purposes does Altmetric LLP intend their metrics?  What they give as a reason for their product is: “Increasingly [researchers] need to show the impact of their papers, books and datasets are having beyond just citations.”  Show to whom?  For what end?

Similarly, CWTS-SURe states that (some of) their goals are:

* “To develop an independent method to retrieve data that analyzes and/or quantifies scientific outputs (not only publications) describing societal quality of research.…”

* “To validate, value, and visualize the results of evaluation of societal quality. In other words, what is the meaning (value) of these societal outputs.”

How would practical impact tie in with their analysis of the “societal quality” of research?

##Required or rewarded

Also, I don't see either Altmetric LLP or CWTS-SURe considering the issue of whether “altmetric value” is going to be required or rewarded.  They seem to be offering them up as supplements to the existent metrics that are being used (rightly or wrongly) to quantify required properties of research.



[brain shutting down… time to get dinner or summat…]






Practical impact

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

What is “practical impact”?  It's when research informs practice.

Practitioners do things.  Researchers study how and why practitioners do the things they do.  Ideally, when researchers identify some previously unknown correlation between what practitioners do and the results they achieve, the practitioners can use that knowledge to improve their ability to get the result they want.

This conversion of knowledge gained by researchers into a tool useable by practitioners is what I'm calling “practical impact”.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Why is the “practical impact” of research interesting?

* On-going empirical research into trivial correlations in social sciences

(Schwab, Andreas, Eric Abrahamson, William H. Starbuck, and Fiona Fidler. 2011. “PERSPECTIVE—Researchers Should Make Thoughtful Assessments Instead of Null-Hypothesis Significance Tests.” *Organization Science* 22 (4): 1105–20. doi:10.1287/orsc.1100.0557.)

As evidenced by Schwab et al. (2011), in the social sciences it is often the case that merely demonstrating that an empirical study achieves statistical significant is considered the same as demonstrating that the study represents knowledge important enough to warrant being published.

However, these days very large data sets are easy to find and even easier to analyze (when compared to earlier parts of the twentieth century, which is when most empirical methods were being developed and popularized), and such data have an inherently massive statistical power.  Meaning that they make it possible to separate even the tiniest of correlations between elements in that data from the statistical “noise” of synchronous yet random variations in the same data.

Such inevitable correlations, while statistically significant, are often trivial when judged by any other measure.  [Schwab talks about…]

What I propose is to judge these inevitable correlations by whether or not they become a tool for use by practitioners.  Especially in the social sciences, where controlled experiments are difficult if not impossible, the best confirmation of both the existence and the importance of a phenomenon is when the knowledge about that phenomenon is taken up by practitioners.

* Lack of relevance in management research

[go mining in my paper for Org Comm.]

* Increasing demand to demonstrate practical value of research by funding agencies

The HEFCE is intending to use the value of research to practitioners as part of its decision process for what research it will recommend gets ongoing government funding.  Other European institutions are doing similar things [pull refs from HEFCE-REF overview paper].

* Value of practical impact may not be captured by citation metrics

As funding agencies begin to track and use information of the practical value of research, research institutions themselves will need to start keeping an eye on the same thing.  Currently in academia, research is most often judged using various bibliometrics, all of which are tied to citations in academic journals.  But citation-based informetrics are almost entirely isolated to academia, and gather no information from practice or practitioners.

So, “practical impact” is not being directly captured by citation-based metrics.  Over the long term, adoption of a given piece of research knowledge will eventually circle back around and increase the citation metrics of the orignal publication, through the citations of researchers working on new studies who observe the knowledge being used in practice, and reference it.

But that cycle will be slow and lossy, as it is dependent on researchers not only recognizing the use of knowledge from previous research, but also recognizing where it came from and citing that source when they publish their own work.

So, citation-based metrics might reflect “practical impact”, but they do not capture it in a timely or accurate manner.


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Tracking practical impact

Altmetrics breaks out of academic silo that isolates most bibliometric methods

“Altmetrics” can refer to either the specific research-metric product from Altmetrics, LLC, or to any metric which uses data from outside of the databases of academic journals used by most, if not all, citation-based metrics.

Both of those sorts of “altmetrics” have the potential to capture some of the practical impact of research, as they are not designed to exclusively use data from academia.  But they also don't exclude data from academia, either, which can obscure evidence of practical impact with the social chatter of academicians.

Similarly, while methods for tracking the impact of research that use data passively provided by both researchers and practitioners (e.g., readership analysis), break out of the academia-only restriction that is inherent with a reliance on rigorous citations for evidence of impact, they also dilute the impact on practitioners in the flood of data from academics.

(Mohammadi, Ehsan, Mike Thelwall, Stefanie Haustein, and Vincent Larivière. 2014. “Who Reads Research Articles? An Altmetrics Analysis of Mendeley User Categories.” *Journal of the Association for Information Science and Technology*, 1–27.)

An example is the study by Mohammadi et al. (2014) on Mendeley readership data.  As the authors explain, “Although the Mendeley API provides information related to the discipline, academic status and country of readers for each record, it only reports percentages rather than raw data and only gives information about the top three categories.”  So, if one of the top three categories of readers isn't a practitioner category, almost no information about an articles impact on practice is available.


CWTS research branch looking for “societal value of research”


HEFCE-REF – requires submission of “cases” demonstrating impact
  Review board for submitted cases comprises both researchers and practitioners


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Uses of practical impact information

Determination of funding (e.g., HEFCE-REF)
Judging “quality of research”
Improving knowledge transfer between researchers and practitioners

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Hazards of making practical value requisite

Lose important research with no practical value
  Research into fundamental laws and principles
  Research into methodologies which enable research that will have practical value

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Hazards of ignoring practical value

“Gaming” system through statistical power [see: Problems]
Losing funding [see: HEFCE-REF]
Rewards that are based only citation-metrics begets research that maximizes citations, and only citations
Is it ethical to leave practitioners ignorant of knowledge that would improve their outcomes?

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Rewarding practical impact

Avoid both
  …the hazard of requiring practical impact, and
  …the hazard of ignoring practical impact


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Categorizing practical impact

Comparisons should always be within the same, or similar, fields
Qualitative process
  Too many possible sources for practical impact data to equitably quantify
  Different fields have widely varying levels of practical impact
Levels:
  U – Unknown/undefined
    No examples of practical impact
  A – Apparent
    At least one example of practical impact
  I – Important
    Multiple examples of practical impact; has more impact than other research in the same field (with at least an A-level)

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Conclusion / Reiteration

Bibliometric methods are well established and widely used, but they reinforce the isolation of academic research from real-world practice.  That isolation has a number of consequences that include: the proliferation of trivial empirical studies; the failure of practitioners to benefit from the knowledge gains of researchers; and the increasingly skeptical scrutiny of the public funding of research.

While requiring practical impact of academic research is both unlikely to be adopted and hazardous to the ongoing advancement of knowledge, the practical impact of research must be rewarded before researchers can be expected to look for and exploit opportunities for achieving practical impact in their research.

Effective reward systems will require effective means of tracking the practical impact of research.  Current bibliometric and altmetric methodologies fail to capture practical impact in anything more than a glancing manner.

An effective practical impact metric could draw data from a number of paths, including social media and readership information.  It would rank the practical impact of research with three non-quantitative levels: Unknown, Apparent and Important.  The distinction between the “A” and “I” levels would be based on comparisons within a field, to avoid disparities between fields that have inherently different potentials for practical impact, and established behaviors that might skew the tracking of practical impact.









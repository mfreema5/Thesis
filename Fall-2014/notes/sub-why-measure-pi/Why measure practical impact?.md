#Why measure practical impact?

[Ian Mitroff (1998)](http://doi.org/10.1177/105649269871011) offers a radical view of the importance of practical impact:

>Pragmatism is the philosophical school that posits that truth is that which makes a significant difference in the lives of humans. Something—an action, empirical finding, proposition, conjecture, theorem—that is true in theory (i.e., in the abstract only) but makes no difference in the lives of humans is not a truth for pragmatism. Thus, contrary to what many academics believe, truth is not solely a property of formal propositions, theorems, research findings, and so forth but of ethical actions (i.e., actions that eliminate, or make significant headway in eliminating, some important human problem).

In other words, without practical impact, a research hypothesis can *never* be “true”, no matter what evidence there may be, empirical or otherwise.

It's not an argument many would agree with, but it's an interesting proposition: Why do we believe that science/research must be independent of practical concerns?

----

Different approach:

If we require research to have practical impact, we end up applying Mitroff's Pragmatism [ref] to research.  We are effectively negating the “truth” of any research that has no practical application by rejecting it.  Only those things “which makes a significant difference in the lives of humans” are accepted, so only they are true.

----

[Charles Hulin (2001)](http://doi.org/10.1111/1464-0597.00055) was writing specifically about the field of I/O psychology when he rejected the notion that research should be required to have practical application, but his arguments are applicable to other fields:

>If, however, a study done on human behaviours in organisations is not read, or is negatively reviewed and consequently does not appear in a journal where it can be read, solely because it does not “inform the practice of psychology” and therefore should not be published, we fetter scientific researchers with the goals of practitioners, goals irrelevant to research.…

>Research and publications with the potential to contribute to an understanding of so fundamental an aspect of our lives as work should be unencumbered by a requirement that they inform practice. An understanding of individuals and work is essential. Such knowledge is also important for a larger arena than applied psychology. The importance of work and a job in defining individuals goes beyond the boundaries of organisations or applied psychology. It informs all behavioural science about a fundamental element of individuals' lives. … We often attempt to apply research findings that are too narrow because we have done research on the symptoms of underlying general problems rather than the problems themselves. The findings may be broad and theoretical and be equally difficult to apply. However, *general solutions come from unfettered basic research*. [emphasis added]

So, we shouldn't let practical impact lead and/or restrict research.

But, remembering Mitroff's radical notions, if existent research findings could be applied in practice to improve outcomes, is it ethical for us to ignore when the are *not* being applied?

>It is hardly news that many organizations do not implement practices that research has shown to be positively associated with employee productivity and firm financial performance (e.g., Hambrick, 1994; Johns, 1993; Pfeffer & Sutton, 2000). Indeed, the failure to implement research-supported practices has been observed in nearly every field where there is a separation between those who conduct research and those who are in a position to implement research findings (Lewis, 2003; Rogers, 1995; Straus, Richardson, Glasziou, & Haynes, 2005).  \[[Rynes, *et al.* (2007)](http://doi.org/10.5465/AMJ.2007.27151939)]

>Sadly, there is poor uptake of management practices of known effectiveness (e.g., goal setting and performance feedback [Locke & Latham, 1984]). Even in businesses populated by MBAs from top-ranked universities, there is unexplained wide variation in managerial practice patterns (e.g., how [or if] goals are set, selection decisions made, rewards allocated, or training investments determined) and, worse, persistent use of practices known to be largely ineffective (e.g., downsizing [Cascio, Young, & Morris, 1997; high ratios of executive to rankand-file employee compensation [Cowherd & Levine, 1992]). The result is a research-practice gap. \[[Rousseau (2006)](http://doi.org/10.2307/20159200)]


It doesn't seem particularly radical to expect that researchers should help practitioners when they can.  But, apparently, they aren't.  So, how can we change that?

[Gary Latham (2007)](http://doi.org/10.5465/AMJ.2007.27153899) gives a simple answer:

>Reward One’s Influence on Practice

>As Mason Haire was fond of saying, that which gets measured gets done. We can fault ourselves for not knowing what is of paramount concern to practitioners, but we cannot fault them for not knowing the usefulness of our research if we do not inform them.

>It is unlikely that many of us will present our findings at practitioner conferences, let alone take time away from our research to write articles for their journals, *if we are not rewarded for doing so* in the academic settings where the majority of us are employed. Deans ignore the recommendations of full professors at their peril. It is we senior faculty who *must change our reward structure*. [emphasis added]

Tracking practical impact is one way to measure the effect that research is having on closing the research-practice gap.

Also, we should consider fields that *do* manage to transfer knowledge between researchers and practitioners.  Unlike in the social sciences, practical impact is implicitly integral to research in the field of medicine.  Why the difference?  [Schwab, *et al.* (2011)](http://doi.org/10.1287/orsc.1100.0557) explain:

>Medical research is more expensive, receives much more funding, makes more of a difference to more people, and draws much more attention. Thus, medical researchers have greater incentive to measure and document effects of their work and to avoid promulgating treatments that turn out later to have been ineffective or harmful.

And [Rynes, *et al.* (2007)](http://doi.org/10.5465/AMJ.2007.27151939) give an additional reason:

>…unlike medicine, education, or law, management is not truly a profession (Leicht & Fennell, 2001; Trank & Rynes, 2003). As such, there is no requirement that managers be exposed to scientific knowledge about management, that they pass examinations in order to become licensed to practice, or that they pursue continuing education in order to be allowed to maintain their practice.

So, to *increase* the practical impact of research, you *measure* the practical impact of research.

Specifically, we should track practical impact in other fields for the same reasons they do in medicine.

* To know which theories/findings practitioners are, or are not, using
  * Which then
    * Lets us explain outcomes
    * Guides future research
    * Allows for follow-up studies on effectiveness

Research does not need to have practical impact to be important or valuable, and requiring practical impact could possibly degrade the overall research in any given field.  But it is unethical to leave practitioners ignorant of practically-applicable research simply because we do not reward researchers for their impact on practice.

----

References-like document: [Importance of practical impact - Collection of quotes](/Fall-2014/notes/Importance of practical impact - Collection of quotes.md)





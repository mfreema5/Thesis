I
  Researchers in many fields can increase the number of publications they have, and therefore the number of citations they receive, by researching any correlation that's statistically “significant”, ignoring all other attributes of the correlation.

    [Refs that prove problem exists.]
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

In an editorial in the <i>Academy of Management Journal</i>, <a href="references.html#combs2010">Combs (2010)</a> questioned whether the ever-more-easy availability of large data sets couple with the ease of performing NHST afford by modern computers and software combine to “mask other shortcomings of our research designs and leave us with itty-bitty effect sizes that limit the relevance of our research. …As management scholars, can we really suggest that managers should change their decision calculus on the basis of knowledge that some new variable explains .0025 percent of the variance in organizational performance?”

Combs went beyond merely speculating on a shift toward larger data sets with smaller effects, and did some comparisons between studies published in the 1987-89 volumes and in the 2007-8 volumes of the <i>Academy of Management Journal</i>.  During that 20-year period, the mean sample size rose from 300 to 3,423 (excluding the three largest samples from the latter period that, if included, would raise the latter mean to 7,578).  Meanwhile, the mean correlations fell from .22, to .17.  The larger samples made smaller and smaller correlations statistically significant (<a href="references.html#combs2010">Combs 2010</a>).

Ellis did a similar survey of empirical studies in the <i>Journal of International Business Studies</i> and found a similar shift toward larger sample sizes, with a similar detrimental effect on relevance of results:

%blockquote
  In many of the studies I read it was apparent that researchers confused statistical with substantive significance when interpreting their results. This usually happened when conclusions about effects were drawn solely by looking at the p-values of test results. The problem with this is that the p-values generated by statistical significance tests are confounded indices that reflect both the size of the effect as it occurs in the population and the sample size used to detect it. As N goes up p goes down, irrespective of the underlying effect size. The implication is that trivial results are sometimes interpreted as meaningful in large-N studies…. <a href="references.html#ellis2010">(Ellis 2010)</a>


<a href="references.html#lockett2014">Lockett <i>et al.</i> (2014)</a> note that the problem of misapplying NHST is not limited to managent research: “Commentators have detailed the problems of employing NHST across a range of different social science disciplines including management, economics and psychology". And, more so, the concerns are not even restricted to the social sciences, as their are “similar concerns in medicine, natural sciences and engineering”.

Lockett, <i>et al.</i>, also point out that NHST has become central to the question of whether or not a paper that presents an empirical study will be published since statistical significance is often (incorrectly) used a proxy for importance. As they put it: NHST “is commonly employed as the sole criterion in deciding on whether or not an effect size is (statistically) significant, with no real consideration of whether or not the effect size is substantively significant (i.e. it really matters)” <a href="references.html#lockett2014">(Lockett <i>et al.</i> 2014)</a>.

They also close the loop, mentioning that since: A - larger sample sizes make even trivial effects statistically significant; and B - statistical significance is central to whether or not a paper gets published; then it follows that “generating a large sample size is an important factor in determining the probability of a paper being published in a top-rated journal”.  This is in spite the fact that “…the impact of the research may be significantly undermined because the results that are deemed important under the criterion of statistical significance may well be trivial in terms of substantive significance” <a href="references.html#lockett2014">(Lockett <i>et al.</i> 2014)</a>.






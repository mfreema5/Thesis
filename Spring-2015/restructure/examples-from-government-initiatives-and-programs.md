Prequel:

* What is an “impact”?
  * Is an impact some specific outcome?
  * Can an impact be quantified?
  * How can you compare the impacts between research?

# THISTHISTHIS

## Examples from government initiatives and programs

### Details of the examples

* “HEFCE launched a pilot exercise in autumn 2009 to trial an impact assessment methodology, which is intended to inform the operational design of an impact- assessment procedure for the proposed Research Excellence Framework (REF) covering the entire UK higher education sector. The assessment of research impact will be one of three distinct elements of the REF, being judged alongside research excellence and research environment, contributing 25% towards the overall outcome (as compared with 60% and 15% for quality and environment).” (Technopolis 2010)



### How has impact been defined:
* In RFQ, Donovan (2008) reports there were competing definitions… 
* In one of the “lessons learned” reports about the HEFCE-REF pilot exercise for higher-education institutions to demonstrate research impact, one of the items given as a challenge was explaining to academic groups “that socio-economic impact was a much broader concept than economic impact” (Technopolis 2010).  The wide breadth of the concept is apparent later in the report, when it states that the pilot exercise would focus on “socio-economic impacts of any type and in any realm, realised outside the academic community” (Technopolis 2010).

* Example 
* Example
 
### How has impact been evaluated:
* The final recommendation for the RFQ was a five-step scale….  As Donovan (2008) reports, though, there was also a three-point scale, that was more inclusive, and less specific to applying research in industry… 
* HEFCE-REF: ‘impact statement’ (?)
* Example
 
### In this context, how has research quality being defined?
* Example
* Example
* Example

### In this context, how has research quality been evaluated? 
* In the RFQ, the quality of research was determined by panels of experts, that were assisted by “quantitative performance measures” (Butler 2008).
* A “suite” of metrics were used.  The working group on metrics specifically recommended “that no attempt be made to aggregate the indicators to produce a single score” (Butler 2008).
  * “Venues”&mdash;meaning journals, conferences, publishers, etc.&mdash;were classified into ranked tiers, and publications evaluated based on the tier of the venue in which they appeared.
  * Citation counts were evaluated in a number of ways: simple counts, averages, centile distributions.  For some areas, citations in “non-standard” venues were included, but discouraged because of the increased labor involved over using established citation indicies.
* A number of metrics were rejected.  The most interesting of which is the ISI Impact Factor.
  * “It was believed that actual citation counts are a far better citation measure for judging the performance of groups than surrogates based on the average citation rates of the journals which carry that work. There were also concerns about the way in which the indicator is calculated and anecdotal evidence of increasing manipulation of the indicator by a few journal editors. Even when ranking journals, some disciplines had already made it clear that they wished to look beyond the Impact Factor and undertake a more detailed assessment of the quality of journals.” (Butler 2008)
* Example
* Example

### How has impact been combined with quality?
* The RFQ relied on panels of experts to evaluate both quality and impact of research.  Each panel had 12 members, 3 of whom represented “end users” of research (Butler 2008).
  * One of the difficulties in combining quality and impact is that the most appropriate time window for each process is not only different from the other, but may not overlap at all.  While fundamental research was exempted from the impact evaluation entirely because of this disconnect in time windows (the impact of fundamental research is indirect, happening through subsequent research, and therefore is even more delayed than non-basic research), Donovan (2008) contends that the window for non-exempted research is still too short to properly capture the impact of the subsequent research.  Therefore, fundamental research is still being devalued despite being exempted from direct evaluation.
* Example
* Example

----

### Other potentially useful text:

* The feedback for the HEFCE-REF pilot exercise for research impact was often positive.  Despite having little experience in documenting or demonstrating impact, “it is clear that HEIs can document non-academic impacts,” but perhaps more importantly “in doing so a great majority will derive insight and local benefits” (Technopolis 2010).
* Participants in the HEFCE-REF pilot exercise reported that, “It did cause departments to take a fresh look at their work, and it shed new light on the breadth of good things that have happened in part as a result of the excellent research being carried out. It has helped people to reflect on their wider contributions…” (Technopolis 2010). The irony is that “The exercise has also revealed how little we do know about the use that is made of our research and equally that where one has a sense of good things happening, we simply don’t record that anywhere. Our evidence base is pretty scant” (Technopolis 2010). The report on the HEFCE-REF pilot exercise echos this irony a number of times: research institutions were unaware of the wider impact they were having despite that impact being real and positive.  E.g., “we were pleasantly surprised at the outcome of the case studies. These clearly provided a much broader appreciation of the impact the university’s research has had / is having than previously recognised” (Technopolis 2010).
* “The growing interest in the notion of research impact, evident amongst all funders…” (Technopolis 2010).
* “Critically, several respondents saw impact assessment as a powerful means by which to show the world how many important, fantastic things universities are doing” (Technopolis 2010).

<br />

* Advice to higher education institutions tasked with demonstrating research impact (Technopolis 2010):  

> * Think about the obvious wider benefits that have derived from research in your specialist area, wherever it was undertaken, and then give some thought to the extent to which you or your group’s work has led to similar sorts of outcomes or possibly even contributed to key steps in these discipline-specific impacts
> * Another institution’s advice was for academics not to limit themselves to recent research, but think in terms of the full range of eligible and underpinning research they have done across their career
> * Another institution said ‘not to panic, we’ve been pleasantly surprised by the whole exercise [and the amount of impact we revealed]’


----

Butler, L. 2008. “Using a Balanced Approach to Bibliometrics: Quantitative Performance Measures in the Australian Research Quality Framework.” *Ethics in Science and Environmental Politics* 8 (June): 83–92. doi:10.3354/esep00077.

Donovan, Claire. 2008. “The Australian Research Quality Framework: A Live Experiment in Capturing the Social, Economic, Environmental, and Cultural Returns of Publicly Funded Research.” *New Directions for Evaluation* 2008 (118): 47–60.

Technopolis Ltd. 2010. *REF Research Impact Pilot Exercise Lessons-Learned Project: Feedback on Pilot Submissions*. Higher Education Funding Council for England. http://www.ref.ac.uk/pubs/refimpactpilotlessons-learnedfeedbackonpilotsubmissions/.


### Evaluation of ‘research quality’ in the examples

#### REF

Much like with ‘practical impact’ (discussed below), the HEFCE ran a pilot exercise of the evaluation of research quality at high-education institutions.  Specifically aiming to validate the use of bibliometric methods, the key finding of the exercise was: “Bibliometrics are not sufficiently robust at this stage to be used formulaically or to replace expert review in the REF” (HEFCE 2009). But this was not a complete rejection of bibliometrics, as “there is considerable scope for citation information to be used to inform expert review” (HEFCE 2009). 

Thus, the REF uses expert panels, comprising both experts in the discipline as well as end-users of research, to evaluate the research quality of an institution, with bibliometric methods being include in the evaluation process. (Grant *et al.* 2010)  

This approach also aligns with the view given in a report commissioned by HECFE before the running of the pilot exercise from the Centre for Science and Technology Studies, Leiden University:

>“Citation counts can be seen as manifestations of intellectual influence, but the concepts of citation impact and intellectual influence do not necessarily coincide. Citation impact is a quantitative concept that can be operationalised in an elementary fashion or in more sophisticated ways, such as crude citation counts versus field-normalised measures. Concepts such as ‘intellectual influence’ are essentially theoretical concepts of a qualitative nature, and have to be assessed by taking into account the cognitive contents of the work under evaluation. Thus, the outcomes of bibliometric analysis must be valued in a qualitative, evaluative framework that takes into account the contents of the work.” (van Raan *et al.* 2007)

Using bibliometrics within a qualitative framework&mdash;such as review by expert panels&mdash;was also the approach in the final version of the RFQ.

#### RFQ

Under the Australian RFQ, research quality would have been assessed by a panel of experts, with one quarter of the members of a panel being end-users of research.  The panel would have used a spectrum of bibliometric measures to compliment the review process.

The government ministry created a ‘Quality Metrics Working Group’, which developed recommendations for what bibliometrics would&mdash;and would not&mdash;be appropriate.  The working group specifically rejected the Journal Impact Factor (né ISI Impact Factor):

>“It was believed that actual citation counts are a far better citation measure for judging the performance of groups than surrogates based on the average citation rates of the journals which carry that work. There were also concerns about the way in which the indicator is calculated and anecdotal evidence of increasing manipulation of the indicator by a few journal editors. Even when ranking journals, some disciplines had already made it clear that they wished to look beyond the Impact Factor and undertake a more detailed assessment of the quality of journals.” (Butler 2008)

The working group on metrics recommended that panels choose from a “suite” of metrics that included citations collated as simple counts, averages, or centile distributions.  Also, it was recommended that some fields might want to include citations from “non-standard venues”, meaning from outside the journals included in the standard indicies from which bibliometrics typically draw data&mdash;though this practice was discouraged, since drawing citations from venues outside those indices would be a labor-intensive process.  And the working group specifically recommended “that no attempt be made to aggregate the indicators to produce a single score” (Butler 2008).

Ostensibly the working group recommended against aggregating quantitative measures to alleviate concerns that a single quantitative measure might have undue influence, but the advice also falls in line with concerns that “analysis of research performance on the basis of journals unavoidably introduces a ‘bibliometrically limited view of a complex reality’” (van Raan *et al.* 2007). 

So, while bibliometrics were to play a role in the RFQ, and do play a role in REF, they are used within a qualitative framework since the evaluation of research is a complex and muli-faceted problem.  As Adler *et al.* write in their criticism of the application of citation statistics:

>“We do not dismiss citation statistics as a tool for assessing the quality of research—citation data and statistics can provide some valuable information. We recognize that assessment must be practical, and for this reason easily derived citation statistics almost surely will be part of the process. But citation data provide only a limited and incomplete view of research quality, and the statistics derived from citation data are sometimes poorly understood and misused. Research is too important to measure its value with only a single coarse tool.” (Adler *et al.* 2009)


----

## References

* Adler, Robert, John Ewing, and Peter Taylor. 2009. “Citation Statistics.” *Statistical Science* 24 (1): 1–14. doi:10.1214/09-STS285.
* Butler, L. 2008. “Using a Balanced Approach to Bibliometrics: Quantitative Performance Measures in the Australian Research Quality Framework.” *Ethics in Science and Environmental Politics* 8 (June): 83–92. doi:10.3354/esep00077.
* Grant, J., P. B. Brutscheer, S. Kirk, L. Butler, and S. Wooding. 2010. “Documented Briefing: Capturing Research Impacts–a Review of International Practice.” *DB-578-HEFCE*. RAND Documented Briefings. RAND Corporation. http://www.rand.org/pubs/documented_briefings/DB578.html.
* HEFCE. 2009. “Report on the Pilot Exercise to Develop Bibliometric Indicators for the Research Excellence Framework.” *HEFCE-REF 2009/39*. Higher Education Funding Council for England (HEFCE). http://webarchive.nationalarchives.gov.uk/20100202100434/http://www.hefce.ac.uk/pubs/year/2009/200939/.
* van Raan, A., H. Moed, and T. van Leeuwen. 2007. “Scoping Study on the Use of Bibliometric Analysis to Measure the Quality of Research in UK Higher Education Institutions.” *HEFCE 2007/34*. http://webarchive.nationalarchives.gov.uk/20120118171947/http://www.hefce.ac.uk/pubs/rdreports/2007/rd18_07/.





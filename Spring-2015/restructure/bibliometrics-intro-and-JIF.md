## Bibliometrics

The bibliometric methods used by other fields as a measure of&mdash;or at least as a surrogate for the measure of&mdash;the impact of research can be divided into two general classes: journal-level metrics, and article-level metrics.

Journal-level metrics rank the journals in field of research.  It is generally assumed that any article published in a journal with a high “impact-factor” is of high quality, and therefore researchers with publications in high impact-factor journals are doing high quality research. 

Article-level metrics are more directly, and aggregate the data about all of an individual's published articles into a single measure, in order to give an indication of the quality of that researcher's work.

### Journal-level metrics

Journal-level metrics are the earliest metrics to become widely used, and continue to be influential.  They measure the impact of journals in a research field using citation counts between journals; the citation cans can be used either directly or indirectly.  Journal-level metrics were originally developed to aid librarians in selecting what journals to which to subscribe and maintain in their collections.  

But journal-level metrics have become a way to indicate the quality of a researchers publications.  Presumably, the logic goes, a journal must publish the best articles in a field in order to have a high impact factor, so therefore any article published in a journal with a high impact factor must be of the best articles.

#### Journal Impact Factor

All citation-based metrics are typically traced back to the paper by Gross and Gross (1927) mentioned above. Archambault and Larivière (2009) tap it as the starting point for the Journal Impact Factor. Since the Journal Impact Factor the best known and possibly most widely used citation-based metric for research, its origins can act as an example for all journal-level metrics in general.

##### Finding the &ldquo;indispensable&rdquo; journals

In 1927, as mentioned above, Gross and Gross (1927) posed the question, “What… scientific periodicals are needed in a college library?” To answer that question of to which journals a library should subscribe, they used quantitative methods to make comparisons between journals, the methods that eventually led to the Journal Impact Factor, and other related bibliometrics.

Gross and Gross decided to use a quantitive method to evaluate journals, as opposed to merely asking an expert to compile a list of &ldquo;indispensable&rdquo; journals in order to avoid a list that was &ldquo;seasoned too much by the needs, likes and dislikes of the compiler.&rdquo;  In other words, they used quantitative methods to minimize any single individual's biases from the evaluation of journals.

##### Counting citations to sort journals

The basic method pioneered by Gross and Gross, and repeated later by other researchers, was to select a pre-eminent journal or a reference that was generally accepted as key to the field at hand, and then compile and quantify the sources cited in that keystone reference/journal.

This is an extremely simplified example: if the journal that was selected as the central reference for a field, &ldquo;Field&rdquo;, contained five citations to a &ldquo;Journal A&rdquo;, ten citations to a &ldquo;Journal B&rdquo;, and four citations to a &ldquo;Journal C&rdquo;, then for that field the journals would be ranked and reported something like this:

<ul>
<li>Leading periodicals in Field

<ul>
<li>Journal B &ndash; 10</li>
<li>Journal A &ndash; 5</li>
<li>Journal C &ndash; 4</li>
</ul></li>
</ul>

These early citation metrics were inherently field-specific, since they took data from a central reference to some particular field.

##### Compiling citations for multiple fields

Soon, though researchers began to cross-compile citation information from multiple journals, and then to include journals from multiple fields in those compilations.  For example, Gregory (1937) compiled more than 27,000 citations from across 27 subfields in medicine.

Gregory gives a concise explanation of the purpose of her metrics:

>The foregoing Tables (I-27) answer primarily the needs of the specialist in his attempt to keep adequately abreast of the literature in his field. Two further Tables have been compiled, to indicate to the medical library and the librarian (A), the indispensable periodicals for al the fields consulted; (B), a short list of essential periodicals in general medicine which cover a large amount of material. This latter list is designed primarily for the individual and for the small library.

There were 27 &ldquo;foregoing Tables&rdquo;, one for each of the subfields, yet in Gregory's Herculean compilation, there were no comparisons made between those fields.  This is a feature that would continue in similar metrics for decades: when multiple fields were compiled and evaluated at the same time, the results for each field were reported separately.  Presumably this was because the intended audience for these metrics had no need for cross-field comparisons, since libraries are generally tasked to select the best journals for some particular field, and are not given carte blanche to choose which fields to support, and researchers are interested in their field, not all fields.

The compilation of citations from Martyn and Gilchrist (1968) is cited by Archambault and Larivière (2009) as having a large influence on methods used in the Journal Impact Factor, and it too continues the practice of reporting of metrics for separate fields separately.

But eventually, when journal impact factors become used as a way to judge the quality of research, this lack of cross-field comparisons will become an issue as institutions try to equitably evaluate the quality of research done by their faculty and staff in an array of fields.  Ways to address this problem are offered by some journal-level as refinements to the methodology of the Journal Impact Factor.  (More details below.)

##### Reporting ratios instead of counts

Another feature of the Martyn and Gilchrist (1968) compilation that was carried forward into the Journal Impact Factor, and other bibliometrics, was first proposed by Hackh (1936): reporting a ratio instead of a count.  Specifically, the ratio of the number of citations to the number of pieces that might be cited.

Consider another extremely simplified example: if there were five citations to articles published in &ldquo;Journal A&rdquo; in the same year that &ldquo;Journal A&rdquo; published a total of 20 articles, the citation ratio would be &ldquo;5:20&rdquo;, or &ldquo;0.25&rdquo;.

We can extend our earlier simplified example to include reporting ratios instead counts, and hopefully see why this is the preferred method:

<table>
<thead>
<tr>
  <th>Journal</th>
  <th>Citation count</th>
  <th>Citable pieces</th>
  <th>Ratio</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Journal A</td>
  <td>5</td>
  <td>20</td>
  <td>0.25</td>
</tr>
<tr>
  <td>Journal B</td>
  <td>10</td>
  <td>20</td>
  <td>0.50</td>
</tr>
<tr>
  <td>Journal C</td>
  <td>4</td>
  <td>10</td>
  <td>0.40</td>
</tr>
</tbody>
</table>
<ul>
<li>Leading periodicals in Field

<ul>
<li>Journal B &ndash; 0.50</li>
<li>Journal C &ndash; 0.40</li>
<li>Journal A &ndash; 0.25</li>
</ul></li>
</ul>

Despite having fewer citations than Journal A, Journal C has I higher ratio citations to citable pieces, and so is ranked higher.  Using that ratio to rank journals is a way to prevent quantity from overwhelming quality.

##### And thus the Journal Impact Factor

This, then, is the core of how the Journal Impact Factor and a number of similar metrics work.  Gather citation counts and citable pieces for each journal, calculate the ratios, and rank the journals.

However, there are a number of potential problems with this method.  Those and some others problems with the Journal Impact Factor are most easily explained by looking at a some other journal-level metrics that use refined versions of this same basic methods.

#### Refinements on the Journal Impact Factor

#### *SNIP* &ndash; Source Normalized Indicator of journal impact per Paper

As mentioned earlier, the lack of cross-field comparisons in the Journal Impact Factor means that its use is problematic for institutions that want to equitably evaluate the quality of research done by faculty and staff in an array of fields.  The Journal Impact Factor of journals cannot be compared between fields “because citation practices can vary significantly from one field to another” (Moed 2011).

The source normalized indicator of journal impact per paper, ‘*SNIP*’, includes refinements to address this problem; specifically it addresses that problem through the normalization of citations.  SNIP normalizes the citation rates of articles against the average number of cited references per paper in that subject field (Moed 2010).  In a simplistic example, if the articles in a field typically cite 10 articles, then a set of articles with a non-normalized citations rate of 12 would have a normalized citation rate of 1.2, which can be characterized as above average *for that field*.

To normalize away the differences in citation practices, however, requires properly delineating where those differences exist; in an article responding to criticisms of SNIP, Moed (2010) cites Garfield (1979) who reported that citation practices differ not only between fields, but between specialties and sub-specialties.

To ensure that a journal's citations are normalized against an appropriate field of research, within SNIP a particular journal's field is not determined by categorization but instead it is defined by what articles cite the journal.  In other words, the “field” is the collection of all articles *outside* the journal that contain citations to articles *inside* the journal.  (It should be noted that the term “article” is being used here loosely.  A more accurate description might be “citable document”, since within SNIP “*articles*, *conference proceedings papers* and *reviews* are considered as fully fledged, peer-reviewed research articles” (Moed 2010).)  In short, SNIP normalizes a journal's citations based on the field's average citations, where the field is defined by citations to the journal.

A version of SNIP that runs against the Scopus&reg; database is available on the Journal Metrics website, `www.journalmetrics.com` (Elsevier 2015).

#### SJR

…


----

## References

* Archambault, Éric, and Vincent Larivière. 2009. “History of the Journal Impact Factor: Contingencies and Consequences.” *Scientometrics* 79 (3): 635–49. doi:10.1007/s11192-007-2036-x.
* Elsevier. 2015. “Journal Metrics: Research Analytics Redefined.” *Journal Metrics: Research Analytics Redefined*. Accessed April 7. http://www.journalmetrics.com/.
* Garfield, Eugene. 1979. *Citation Indexing - Its Theory and Application in Science, Technology, and Humanities*. New York: Wiley.
* Gregory, Jennie. 1937. “An Evaluation of Medical Periodicals.” *Bulletin of the Medical Library Association* 25 (3): 172–88.
* Gross, P. L. K., and E. M. Gross. 1927. “College Libraries and Chemical Education.” *Science* 66 (1713): 385–89.
* Hackh, Ingo. 1936. “The Periodicals Useful in the Dental Library.” *Bulletin of the Medical Library Association* 25 (1-2): 109–12.
* Martyn, John, and Alan Gilchrist. 1968. *An Evaluation of British Scientific Journals*. London: Aslib.
* Moed, Henk F. 2010. “Measuring Contextual Citation Impact of Scientific Journals.” *Journal of Informetrics* 4 (3): 265–77. doi:10.1016/j.joi.2010.01.002.
* Moed, Henk F. 2011. “The Source Normalized Impact per Paper Is a Valid and Sophisticated Indicator of Journal Citation Impact.” *Journal of the American Society for Information Science and Technology* 62 (1): 211–13. doi:10.1002/asi.21424.

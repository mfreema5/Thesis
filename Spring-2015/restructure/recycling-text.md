# Recycling text from old version of &lsquo;tawp&rsquo;

This is a copy-n-paste of chunks of text that I'd written earlier.  Messy and incoherent, but a place to start.

## How we judge the relative value of research &ndash; current

### Journal-level metrics

#### Journal Impact Factor

<article id="explaining-jif">
  <h2>
    What is a &ldquo;citation-based metrics such as the Journal Impact Factor&rdquo;?
  </h2>
  <section>
    <h3>
      Explaining the Journal Impact Factor through its historical foundations
    </h3>
    <p>
      To help explain what the Journal Impact Factor metric is&mdash;or, more exactly, what it does&mdash;we can trace its origins.  This history-based explanation draws heavily on: <a href="references.html#archambault2009">Archambault, Éric, and Vincent Larivière. 2009.</a> &ldquo;History of the Journal Impact Factor: Contingencies and Consequences.&rdquo; <i>Scientometrics</i> 79 (3): 635–49.
    </p>
  </section>
  <section>
    <h3>
      Finding the &ldquo;indispensable&rdquo; journals
    </h3>
    <p>
      Almost a hundred years ago, <a href="references.html#gross1927">Gross and Gross (1927)</a> posed the question:
    </p>
    <blockquote>
      What… scientific periodicals are needed in a college library successfully to prepare the student for advanced work, taking into consideration also those materials necessary for the stimulation and intellectual development of the faculty?
    </blockquote>
    <p>
      To answer that question of to which journals a library should subscribe, they used quantitative methods to make comparisons between journals.  Their methods eventually led to the Journal Impact Factor, and other related bibliometrics.
    </p>
    <p>
      Gross and Gross used a quantitive method to evaluate journals, as opposed to merely asking an expert to compile a list of &ldquo;indispensable&rdquo; journals in order to avoid, as they described it, a list that was &ldquo;seasoned too much by the needs, likes and dislikes of the compiler.&rdquo;  In other words, their goal was to achieve objectivity in the evaluation by using quantitative methods.
    </p>
  </section>
  <section>
    <h3>
      Counting citations to sort journals
    </h3>
    <p>
      The basic method pioneered by Gross and Gross, and repeated later by other researchers, was to select a pre-eminent journal or a reference that was generally accepted as key to a field, and then to compile and quantify the sources of the citations in that keystone reference/journal.
    </p>
    <p>
      An extremely simplified example: if the journal that was selected as the central reference for a field, &ldquo;Field&rdquo;, contained five citations to a &ldquo;Journal A&rdquo;, ten citations to a &ldquo;Journal B&rdquo;, and four citations to a &ldquo;Journal C&rdquo;, then for that field the journals would be ranked and reported something like this:
    </p>
    <ul>
    <li>Leading periodicals in Field

    <ul>
    <li>Journal B &ndash; 10</li>
    <li>Journal A &ndash; 5</li>
    <li>Journal C &ndash; 4</li>
    </ul></li>
    </ul>
    <p>
      These early citation metrics were inherently field-specific, since they took data from a central reference to some particular field.
    </p>
  </section>
  <section>
    <h3>
      Compiling citations for multiple fields
    </h3>
    <p>
      Soon, though researchers began to cross-compile citation information from multiple journals, and then to include journals from multiple fields in their compilations.  For exxample, <a href="references.html#gregory1937">Gregory (1937)</a> compiled more than 27,000 citations from across 27 subfields in medicine.
    </p>
    <p>
      Gregory gives a concise explanation of the purpose of her metrics:
    </p>
    <blockquote>
      The foregoing Tables (I-27) answer primarily the needs of the specialist in his attempt to keep adequately abreast of the literature in his field. Two further Tables have been compiled, to indicate to the medical library and the librarian (A), the indispensable periodicals for al the fields consulted; (B), a short list of essential periodicals in general medicine which cover a large amount of material. This latter list is designed primarily for the individual and for the small library.
    </blockquote>
    <p>
      There were 27 &ldquo;foregoing Tables&rdquo;, one for each of the subfields.  No comparisons were made between those fields.  This is a feature of similar metrics that would continue for decades: even when multiple fields were measured at the same time, the results for each field were reported separately.  The intended audience for these metrics had no need for cross-field comparisons.  Libraries are generally tasked to select the best journals for some particular field, and are not given carte blanche to choose which fields to support.  Similarly, researchers are interested in their field, not all fields.
    </p>
    <p>
      The compilation of citations from <a href="references.html#martyn1968" >Martyn and Gilchrist (1968)</a> is cited by <a href="references.html#archambault2009" >Archambault and Larivière (2009)</a> as having a large influence on methods used in the Journal Impact Factor, and it continues the practice of separating the reporting of metrics for separate fields.
    </p>
  </section>
  <section>
    <h3>
      Reporting ratios instead of counts
    </h3>
    <p>
      Another feature of the Martyn and Gilchrist compilation, was was carried forward into the Journal Impact Factor and other bibliometrics, was first proposed by <a href="references.html#hackh1936">Hackh (1936)</a>: the reporting of a ratio of the number of citations to the number of pieces that might be cited.
    </p>
    <p>
      Another extremely simplified example: if there were five citations to articles published in &ldquo;Journal A&rdquo; in the same year that &ldquo;Journal A&rdquo; published a total of 20 articles, the citation ratio would be &ldquo;5:20&rdquo;, or &ldquo;0.25&rdquo;.
    </p>
    <p>
      We can extend our example to include comparing journals.
    </p>
    <table>
      <thead>
        <tr>
          <th>Journal</th>
          <th>Citation count</th>
          <th>Cite-able pieces</th>
          <th>Ratio</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Journal A</td>
          <td>5</td>
          <td>20</td>
          <td>0.25</td>
        </tr>
        <tr>
          <td>Journal B</td>
          <td>10</td>
          <td>20</td>
          <td>0.50</td>
        </tr>
        <tr>
          <td>Journal C</td>
          <td>4</td>
          <td>10</td>
          <td>0.40</td>
        </tr>
      </tbody>
    </table>
    <ul>
    <li>Leading periodicals in Field

    <ul>
    <li>Journal B &ndash; 0.50</li>
    <li>Journal C &ndash; 0.40</li>
    <li>Journal A &ndash; 0.25</li>
    </ul></li>
    </ul>
    <p>
      This is the core of how the Journal Impact Factor and a number of similar metrics work.
    </p>
  </section>
  <section>
    <h3>
      Selecting what years to include
    </h3>
    <p>
      There is an important, but subtle, characteristic of compilations of citations left to be described, which has been carried forward all the way from the earliest forms to the Journal Impact Factor and other contemporary metrics.  But to explain it, we must first look at an important but obvious charactertisic: the time &ldquo;window&rdquo; within which citations must occur in order to be included in a compilation's counts.
    </p>
    <p>
      From the begining there was a time restriction around the citations included.  The <a href="references.html#gross1927">Gross and Gross (1927)</a> compilation only considered citations from 1926, because they drew all of their citations from the 1926 volume of <i>The Journal of the American Chemical Society</i>, which at the time was &ldquo;the most recent complete volume&rdquo;.  Other early metrics used restricted windows of time for similarly practical reasons.
    </p>
    <p>
      <a href="references.html#martyn1968">Martyn and Gilchrist (1968)</a> also give a practical reason for the restricted window of time in their citation metrics: &ldquo;the two-year sample reduced the effort of counting and also reduced the cost of acquisition of the data&rdquo;.  But they preface that with an interesting justification:
    </p>
    <blockquote>
      We decided that our most practical course would be to confine our study to citations made during 1965 to journals published in the two preceding years. It was already known that 26.1% of all the 1965 citations were to literature of 1964 and 1963, so in terms of number of citations this would give us an adequate sample.
    </blockquote>
    <p>
      Their method was not meant to be comprehensive.  In other words, it did not capture the entire population of citations, or pieces that could be cited.  Instead, the method operates on a sample of citations, and publications.
    </p>
  </section>
  <section>
    <h3>
      Samples, not populations, of citations
    </h3>
    <p>
      That these metrics are based on samples&mdash;as opposed to entire populations&mdash;is the important but non-obvious characteristic of citation compilations mentioned above.
    </p>
    <p>
      It's an important feature because many of the criticisms of the Journal Impact Factor can be viewed as simply issues related to the <i>sampling method</i> of the metric.  (What time window?  Which journals?  Which published pieces?)  And many of the contemporary bibliometric metrics that are offered as improvements to the Journal Impact Factor primarily differ from it only in the sampling methods employed.  Other than that they return the same citation-based analysis as the Journal Impact Factor.
    </p>
  </section>
  <section>
    <h3>
      So, what's a JIF, anyway?
    </h3>
    <p>
      In short: the Journal Impact Factor, and similar metrics:
    </p>
    <ul>
    <li>Sample citations in various journals</li>
    <li>Compare the number of actual citations to the number of potential citations</li>
    </ul>
    <p>
      The inherent (and unquestioned) assumption in all of this is that citation counts are strongly correlated with some attribute of research that is desirable.  It might be &ldquo;importance&rdquo;, or perhaps &ldquo;quality&rdquo;, or (inexactly) &ldquo;significance&rdquo;.  The nominal description of this attribute is &ldquo;Impact&rdquo;.
    </p>
  </section>
</article>

#### Refinements on JIF

##### SNIP

>“This paper explores a new indicator of journal citation impact, denoted as source normalized impact per paper (SNIP). It measures a journal’s contextual citation impact, taking into account characteristics of its properly defined subject field, especially the frequency at which authors cite other papers in their reference lists, the rapidity of maturing of citation impact, and the extent to which a database used for the assessment covers the field’s literature….”

>“SNIP is defined as the ratio of the journal’s citation count per paper and the citation potential in its subject field. It aims to allow direct comparison of sources in different subject fields.” (Moed 2010)

SNIP corrects for differences between fields and topics within fields, by addressing:

* Citation behaviors
* Size of field/topic
* Citations made to items not counted as citable

SNIP doesn't address any other problems with the JCR's Journal Impact Factor.

One interesting feature is that it doesn't rely on the pre-categorization of journals into fields or topics.  It uses the inter-relationship of citations to identify the context of an (article|journal).

##### SJR

>“The SJR indicator is a size-independent metric aimed at measuring the current "average prestige per paper" of journals for use in research evaluation processes” that is “based on eigenvector centrality.” (González-Pereira 2010)

 ###### “eigenvector centrality”

Lots of mathematical jargonism in the explanation of this in González-Pereira (2010).  As [Inigo Montoya](http://www.imdb.com/character/ch0003786/quotes) said, “Let me explain. … No, there is too much. Let me sum up.”

Every journal is a node, each citation in that journal makes a connection to the node of the journal containing the article being cited.  This mythical stuff “prestige” is shifted between nodes based on the strength of the connection between the two nodes.  The strength of the connection is determined by the what fraction of the total of all citations are included in that connection.

----

Math digression:

* Three nodes (`A`, `B` and `C`) with 34 citations between them:

| node ➜ node | citation count | percentage of total |
|--------------|----------------|---------------------|
|   A ➜ B     |  3             |   8.8 %             |
|   A ➜ C     |  9             |  26.5 %             |
|   B ➜ A     |  5             |  14.7 %             |
|   B ➜ C     |  5             |  14.7 %             |
|   C ➜ A     |  7             |  20.6 %             |
|   C ➜ B     |  5             |  14.7 %             |

So `A` has to give more “prestige” to `C` than to `B`.  `B` gives to `A` and `C` equally, while `C` gives more to `A` than to `B`.  The end result is that `B`'s prestige migrates to `A` and `C`, with more going to `C`.  So, the nodes would be ranked: `C`, `A`, `B`.

----

You guess at a prestige for all the nodes, then start iterating through shifts of prestige until you sink into a minima.  (Do they know if it's a global or local minima?)  You rank the journals based on the resultant prestige of their nodes.

###### Advantages of SJR over JIF:

* Not all citations count the same; instead their weighted by the “prestige” of the journal in which they occur.
  * “…The SRJ indicator establishes different values for citations according to the scientific influence of the journals that generate them.” (González-Pereira 2010)
* Self-citations are capped.
  * “It restricts a journal's self-citation to a maximum of 33% of its issued references, so that excessive self-citation will not involve artificially inflating a journal's value, but without touching the normal process of self-citation.” (González-Pereira 2010)
    * *(One third seems arbitrary.  And any editor with a brain will make sure there are  AT LEAST that many self-citations in their journal.  Does setting a ceiling always create a target?)*

**LH NOTE:** *need some element of time here. Without it, not sure how the nodes can cite each other (i.e., in order to be cited something has to exist). Plenty of network measures of prestige - how is this one different?*


### Article-level metrics

#### h-Index

Hirsch, J. E. 2005. “An Index to Quantify an Individual’s Scientific Research Output.” *Proceedings of the National Academy of Sciences of the United States of America* 102 (46): 16569–72. doi:[10.1073/pnas.0507655102](http://doi.org/10.1073/pnas.0507655102).

* an actor has an *h*-index of “h” when
  * they have “h” number of papers that
    * have ≥ “h” number of citations
  * E.g., 5 papers with 5 or more citations.

#### Altmetrics

##### “Manifesto”

“No one can read everything.”  (Priem, J., Taraborelli, D., Groth, P., & Neylon, C. 2010. *alt-metrics: A manifesto.* http://altmetrics.org/manifesto/)

Traditional filters:
* Peer review process for publication
* Citation counts
* Journal rankings

Problems with those:
* Slow
* Discourage innovation / encourage *status quo*
* Lack of accountability for peer-reviewers
* Peer-review doesn't *prevent* publication, it just pushes lower-value articles to lower-prestige journals (in ideal circumstances); those articles are still in the population.
* Too narrow in what they count, index, or include
* Too focused within academia
* Ignore context of citations (a critical citation counts as much as a positive one)
* Indicies are mis-used to assign funding and evaluate personnel
* Many are easy to game

Claims:
* “Altmetrics look beyond counting and emphasize semantic content.”
* “…Altmetrics reflect the impact of the article itself, not its venue.
  * *(Somewhat specious, since the venue typically has a major effect on the impact.)*
* Altmetrics will include the previously ignored:
  * “impact outside the academy”
  * “impact of influential but uncited work, and”
  * “impact from sources that aren't peer-reviewed.”

----

####Active versus passive citations
They never mention what I see as a major difference between data that comes from things like downloads and reference-manager information, and the traditionally-used citations data.  Which is that the former tracks the unconsidered assignment of importance.  Meaning, you consider what sources to cite; you have preferences about what sources to use, and which to avoid.  You may (or more likely: “should”) adjust what sources you cite based on the audience you expect, or based on to which journal you'll be submitting something, or even whom you expect to receive it for reviewing.

What papers you download to read/use or whatever aren't nearly as considered.  You may hate some sonofabitch and avoid citing him as much as you can, but if he does good work, you'll download his papers.

Though, you may download them to look for things to criticize. But that probably doesn't matter.  It's probably best *not* to differentiate between “positive” and “negative” citations.  Either way, you considered the thing being cited as more important than all the other things that you might have been cited in its place.

But back to the main point here: passive citations seem potentially more “honest” than active ones.  Less “gamed” on a individual level.

----

<section id="altmetrics-overview">
  <h3>
    Altmetrics - overview
  </h3>
  <p>
    According to <a href="references.html#zahedi2014">Zahedi <i>et al.</i> (2014)</a> alternative metrics include, “usage data analysis (download and view counts); web citation, and link analyses.”  An even more general description is that altmetrics are intended to use data available on the web to supplement and improve upon citation-based metrics for measuring the impact of science and research.  Through the access to information made available to the web, altmetrics can go beyond the journal articles and books included in citation-based metrics to include “other outputs such as datasets, software, slides, blog posts, etc.” <a href="references.html#zahedi2014">(Zahedi <i>et al.</i> 2014)</a>.
  </p>
  <p>
    Instead of compiling citations to research in journals, altmetrics involves compiling “mentions” of research in “main-stream media sources, and social media shares and discussions”, along with statistics on downloads, and reference manager counts (<a href="references.html#adie2013">Adie and Roe 2013)</a>.
  </p>
  <p>
    Altmetrics also typically retain the related meta-data of mentions and usage statistics, which allow for more complex analyses of the information.  For example, <a href="references.html#mohammadi2014">Mohammadi et al. (2014)</a> studied the relationship between citation metrics and the usage statistics and user information available from a reference manager service, and found some insights into the the limitations of citation-based metrics.
  </p>
  <p>
    So, altmetrics not only track what research is being “mentioned”, but also where it is mentioned, and who is mentioning it.
  </p>
</section>

### Other approaches

* Cited Half-life
  * Tracks *when* citations “peak” to indicate nature of impact
* Immediacy Index of the ISI
  * Similar to JIF, but more recent time window
* PageRank
  * Variant of “eigenvector centrality” algorithm(?)

### Summaries of existing metrics

#### Journal-level metrics

| TLA  |  Name                              |  Data source(s)                                       | More info |
|------|------------------------------------|-------------------------------------------------------|-----------|
| JIF  | Journal Impact Factor              | [Web of Science](http://wokinfo.com/about/mjl/)       | [Thomson Reuters](http://thomsonreuters.com/journal-citation-reports/)  |
| II-WoS | Immediacy Index                  | [Web of Science](http://wokinfo.com/about/mjl/)       |  |
| JFIS | Journal-to-Field Impact Score      | [Web of Science](http://wokinfo.com/about/mjl/)       | CWTS  |
| SNIP | Source Normalized Impact per Paper | [Scopus](http://www.elsevier.com/online-tools/scopus) | [CWTS Journal Indicators](http://www.journalindicators.com/methodology) |
| RIP  | Raw Impact per Publication         | [Scopus](http://www.elsevier.com/online-tools/scopus) | [CWTS Journal Indicators](http://www.journalindicators.com/methodology) |
| SJR  | SCImago Journal Rank               | [Scopus](http://www.elsevier.com/online-tools/scopus) | [SCImago Journal & Country Rank](http://www.scimagojr.com/)             |

Note:
* Institute for Scientific Information (ISI) *begat*
  * ISI Web of Knowledge *begat*
    * Thomson Scientific [Web of Science](http://thomsonreuters.com/thomson-reuters-web-of-science/)
      * Science Citation Index
      * Social Science Citation Index
      * Arts & Humanities Citation Index

#### Article-level metrics

| TLA    |  description                 |  Data source(s)                                       | More info |
|--------|------------------------------------|-------------------------------------------------------|-----------|
| *h*-I  | *h*-index                          | (*multiple possible?*)                                | Hirsch, J. E. 2005                                                       |

  * Hirsch, J. E. 2005. “An Index to Quantify an Individual’s Scientific Research Output.” *Proceedings of the National Academy of Sciences of the United States of America* 102 (46): 16569–72. doi:[10.1073/pnas.0507655102](http://doi.org/10.1073/pnas.0507655102).


##### CWTS (article-level) &ldquo;indicators&rdquo;

| TLA           |  description                                           |
|---------------|--------------------------------------------------------|
| **P**         | number of publications by [actor]                      |
| **C**         | number of received citations                           |
| **CPP**       | mean number of citations per pub                       |
| **Pnc**       | percentage of pubs not cited                           |
| **% SC**      | percentage self-citations related to an output set     |
| **CPP/JCSm**  | ratio (mean actor cites/pub) to (mean journal impact)  |
| **CPP/FCSm**  | ratio (mean actor cites/pub) to (mean field impact)    |
| **JCSm/FCSm** | ratio (journal impact) to (field impact)               |


#### Some features of both journal- and article-level metrics

| index / indicator | article-level | silo'd | mono-db |
|-------------------|---------------|--------|---------|
| JIF               | N             |  Y     |  Y      |
| II-WoS            | N             |  Y     |  Y      |
| JFIS              | N             |  Y     |  Y      |
| SNIP              | N             |  Y     |  Y      |
| RIP               | N             |  Y     |  Y      |
| SJR               | N             |  Y     |  Y      |
| *h*-Index         | Y             |  Y     |  N      |
| **P**             | Y             |  Y     |  N      |
| **C**             | Y             |  Y     |  N      |
| **CPP**           | Y             |  Y     |  N      |
| **Pnc**           | Y             |  Y     |  N      |
| **% SC**          | Y             |  Y     |  N      |
| **CPP/JCSm**      | Y             |  Y     |  N      |
| **CPP/FCSm**      | Y             |  Y     |  N      |
| **JCSm/FCSm**     | Y             |  Y     |  N      |

* “silo'd” - Is the metric restricted to academia?
* “mono-db” – Does it use one and only one database?


### Problems with this/these

#### Lack of relevance

#### &ldquo;Seduced by the apparent objectivity of numbers&rdquo;


## How we judge the relative value of research &ndash; expected future includes Practical Impact

### What is practical impact

#### Do current metrics capture practical impact

<section>
  <h3>
    Value of practical impact may not be captured by citation metrics
  </h3>
  <p>
    As funding agencies begin to track and use information of the practical value of research, research institutions themselves will need to start keeping an eye on the same thing.  Currently in academia, research is most often judged using various bibliometrics, all of which are tied to citations in academic journals.  But citation-based informetrics are almost entirely isolated to academia, and gather no information from practice or practitioners.
  </p>
  <p>
    Which means that “practical impact” is not being directly captured by citation-based metrics (<a href="references.html#mohammadi2014">Mohammadi <i>et al.</i> 2014</a>).  Over the long term, adoption of a given piece of research knowledge will eventually circle back around and increase the citation metrics of the orignal publication, through the citations of researchers working on new studies who observe the knowledge being used in practice, and reference it.
  </p>
  <p>
    But that cycle will be slow and lossy, as it is dependent on researchers not only recognizing the use of knowledge from previous research, but also recognizing where it came from and citing that source when they publish their own work.
  </p>
  <p>
    So, citation-based metrics such as the Journal Impact Factor might reflect “practical impact”, but they do not capture it in a timely or accurate manner.
  </p>
</section>
<section id="altmetrics-examples-related-to-practical-impact">
  <h3>
    Altmetrics - examples related to practical impact
  </h3>
  <p>
    For an example of possible ways to identify practical impact using altmetrics, we'll return to the study by <a href="references.html#mohammadi2014">Mohammadi <i>et al.</i> (2014)</a>, in which they analyzed data from Mendeley, an on-line service and software package for managing personal libraries of research papers.
  </p>
  <p>
    What makes the Mendeley data potentially useful for identifying possible practical impact is that the meta-data includes the user's “profession”.  Users self-select that from a pre-populated list of options, so it's not infallible, but it does allow for classing users as being research academics (those who author citable research), non-research academics (those who don't author papers&mdash;e.g., students), or someone outside of academia.  The research preferred by the class of users who are not members of academia would presumably be the research that is most relevant to practitioners.
  </p>
  <p>
    As Mohammadi <i>et al.</i> put it:
  </p>
  <blockquote>
    …It seems that Mendeley readership is able to provide evidence of using research articles in contexts other than for their science contribution, at least for Social Science and some applied sub-disciplines. …It also could be used as a supplementary indicator to measure the impact of some technological or medical papers in applied contexts, as citation analysis is more useful for the assessment of theoretical research rather than applied research. <a href="references.html#mohammadi2014">(Mohammadi <i>et al.</i> 2014)</a>
  </blockquote>
  <p>
    Where “contexts other than for their science contribution” and “the impact… in applied contexts” are presumably equivalent to practical impact.
  </p>
  <p>
    Though, we should also note one of the caveats given by the authors: “Mendeley is perhaps most useful for those who will eventually cite an article and so its readership counts seem likely to under-represent users who will never need to cite an article, perhaps including disproportionately many practitioners” <a href="references.html#mohammadi2014">(Mohammadi <i>et al.</i> 2014)</a>.  So, while the data from Mendeley suggests the potential of tracking practical impact, it may not be able to fulfill it.
  </p>
  <p>
    Similarly, the various examples of altmetric data given by <a href="references.html#adie2013">Adie and Roe (2013)</a> hint at the potential for extracting practical impact information from social media sources, while also highlighting some of the difficulties in utilizing those sources.  E.g., though they were able to collect meta-data about the users involved in “mentions” of citable research, there's no indication that there is any data available which would allow users to be classified as being, or not being, members of academia.
  </p>
  <p>
    But, then again, this lack of meta-data suitable for classifying users is only a hinderance to the automated analysis and compilation of data.  Finding and flagging altmetric mentions in social-media discussion could still potentially provide usable information about the practical impact of research, if it could be followed by a “manual” analysis of the context of the discussion.  For example, Altmetric LLC provides for this process by using “data sources that can be manually audited by our users. If Altmetric says that an article has been tweeted about five times, then users should be able to get the relevant five links, Twitter usernames, and timestamps to go check for themselves” <a href="references.html#adie2013">(Adie and Roe 2013)</a>. And if it were to a researcher's advantage to be able to demonstrate the practical impact of their research, it is would also be to that researcher's advantage to provide the resources needed to interpret the altmetric data related to their research.
  </p>
</section>

### Examples of ways to evaluate pratical impact

#### HEFCE-REF

<p>
  The HEFCE is intending to use the value of research to practitioners as part of its decision process for what research it will recommend gets ongoing government funding.  Other European institutions are doing similar things [pull refs from HEFCE-REF overview paper].
</p>

#### Austrialian prototype

#### U.S. PART program (name?)

### How the examples worked (details)

#### Case studies

#### Evaluation panels (which include practitioners)

## Future research

## Conclusion

<p>
  Bibliometric methods are well established and widely used, but they reinforce the isolation of academic research from real-world practice.  That isolation has a number of consequences that include: the proliferation of trivial empirical studies; the failure of practitioners to benefit from the knowledge gains of researchers; and the increasingly skeptical scrutiny of the public funding of research.
</p>
<p>
  While requiring practical impact of academic research is both unlikely to be adopted and hazardous to the ongoing advancement of knowledge, the practical impact of research must be rewarded before researchers can be expected to look for and exploit opportunities for achieving practical impact in their research.
</p>

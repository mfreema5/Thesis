##### And thus the Journal Impact Factor

This, then, is the core of how the Journal Impact Factor and a number of similar metrics work.  Gather citation counts and citable pieces for each journal, calculate the ratios, and rank the journals.

However, there are a number of possible problems with this method.  For example, the Journal Impact Factor has been criticized for the following reasons (among others):

*The Journal Impact Factor…*

  *…is not sensitive to differences in composition of the journals.*  Review pieces in journals that aggregate recent research into an overview nearly always have higher citation rates than research articles.  Letters and notes tend to have higher citation rates in the short-term.  Differing journal impact factors may therefore represent nothing more than a difference in number of each type of citable documents in the journals (van Leeuwen *et al.* 1999).

  *…uses too short of a citation window.* For subfields of mathematics, social sciences and humanities “the use of an indicator which depends on a citation window of one to two years seems to be clearly insufficient to measure the impact of research in those areas” (van Leeuwen *et al.* 1999).

  *…is is not sensitive to differences in fields.* The Journal Impact Factor does not account for differences in “the size of professional communities, the numbers of their indexed journals and type of articles in different fields, and the researchers’ differing citing behaviours” and therefore, impact factors cannot be appropriately compared between fields. (Bornmann *et al.* 2012)

So while the Journal Impact Factor remains popular, other journal-level metrics have been developed that either address specific problems or implement newer methodologies, e.g., prestige rankings.  Three examples of those are described in the following section.

----

## References

* Bornmann, Lutz, Werner Marx, Armen Yuri Gasparyan, and George D. Kitas. 2012. “Diversity, Value and Limitations of the Journal Impact Factor and Alternative Metrics.” *Rheumatology International* 32 (7): 1861–67. doi:10.1007/s00296-011-2276-1.

* van Leeuwen, Thed N., H. F. Moed, and J. Reedijk. 1999. “Critical Comments on Institute for Scientific Information Impact Factors: A Sample of Inorganic Molecular Chemistry Journals.” *Journal of Information Science* 25 (6): 489–98. doi:10.1177/016555159902500605.


### Conceptual issues with citation analysis

It's important to remember the origins of the citation-based journal-level metrics, which are also the conceptual ancestors of citation-based article-level metrics.  Specifically, what's important is that the citation-based metrics were developed to aid librarians in deciding what journals would be most useful to their patrons (Gross and Gross 1927, Archambault and Larivière 2009).  And in research libraries the patrons are likely to be among the authors of the articles in those journals; in other words, the patrons are likely to be the people who make citations in the first place.  So, prioritizing journals by citation rates makes sense, since the journals most likely to contain citable documents in the future are those that contained citable documents in the past.

The transition from using citation-based metrics for selecting what journals to carry in a library to using those metrics for evaluating the “research impact” of articles was based on the assumption that the most important research will necessarily get cited the most.  Yet this is obviously not always true.  For example, the article “Electrochemically induced nuclear fusion of deuterium” (Fleischmann and Pons 1989) received 490 citations in its first four years after publication, but ‘cold fusion’ (as it came to be know) had little actual research impact (Moed 2005).

So, citation-based metrics do not account for the nature of citations&mdash;a negative citation counts the same as a positive one.  To further complicate matters, the ability to clearly distinguish a negative from a positive citation, and the ability to state that a negative citation doesn't represent a net positive influence in the field, are dependent on how you define “intellectual influence”.  Indeed, this dependence extends to citation-based metrics in general:

> “The citation impact of [a] work can be interpreted in terms of intellectual influence. If one disregards the permanence of the intellectual influence, its cognitive direction and its longer term implications, this concept becomes more similar to that of citation impact. On the other hand, if an evaluator considers these aspects of intellectual influence as important attributes in a qualitative assessment, discrepancies between a work’s citation impact and the assessment of its intellectual influence are apt to rise.” (Moed 2005)

This helps explain why, in the examples of the evaluation of research impact in later sections, expert panels shied away from using citation-based metrics as anything more than an ‘indicator’ of research impact.  “In order to be useful and properly used in research evaluation, citation impact must be further interpreted” within a framework that is appropriate for the field, and the particular research in question (Moed 2005).  This is what was proposed for the Australian Research Quality Framework, and what was implemented in the United Kingdom's Research Excellence Framework, which are described in the sections on practical impact that follow.

----

# References

* Archambault, Éric, and Vincent Larivière. 2009. “History of the Journal Impact Factor: Contingencies and Consequences.” *Scientometrics* 79 (3): 635–49. doi:10.1007/s11192-007-2036-x.

* Fleischmann, Martin, and Stanley Pons. 1989. “Electrochemically Induced Nuclear Fusion of Deuterium.” *Journal of Electroanalytical Chemistry and Interfacial Electrochemistry* 261 (2): 301–8. doi:10.1016/0022-0728(89)80006-3.

* Gross, P. L. K., and E. M. Gross. 1927. “College Libraries and Chemical Education.” *Science* 66 (1713): 385–89.

* Moed, H. F. 2005. *Citation Analysis in Research Evaluation*. Dordrecht; [Great Britain]: Springer.


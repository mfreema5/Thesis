# The move to include practical impact in evaluation research

## Examples from government initiatives and programs

### Background on the examples  

Both Australia and the United Kingdom have developed government programs that evaluate research institutions based on their “research quality” and “research impact”.  Those programs used “research quality” to refer to what is sometimes called the “academic impact” of research; it is a measure of the effect that research has on other research, and included some form of bibliometrics.

“Research impact” is the effect that research has *outside* of research and research institutions; it is an evaluation of the “full range of economic, social, public policy, welfare, cultural and quality-of-life benefits” (Grant *et al.* 2009) that can result from research.  In this thesis, it is referred to as “practical impact”.

#### Research Quality Framework (RQF)  

The history of the Research Quality Framework (RQF) is complicated.  It started in 2003 when the government established “an Expert Advisory Group, whose remit was to consult widely and develop a model for assessing the quality and impact of research in Australia” (Butler 2008).  The framework proposed by that group was published in 2006, and prompted the establishment of the Development Advisory Group, to “refine the RQF model” (Donovan 2008).  The first RFQ was scheduled to be run in 2008, but a change in government in 2007 instead meant that the program was scrapped (Donovan 2008).

However, the Research Quality Framework has influenced the development of similar programs, such as the Research Excellence Framework in the UK (Grant *et al.* 2009).

#### Research Excellence Framework (REF)  

The Higher Education Council for England “funds and regulates universities and colleges in England” (HEFCE 2015).

The HEFCE will be using the Research Excellence Framework (REF) “for the assessment and funding of research in UK higher education institutions”. The REF focuses on three elements (HEFCE 2009):

1. **Research quality** &ndash; research “will be assessed against criteria of ‘rigour, originality and significance’. By ‘significance’, we mean the extent to which research outputs display the capacity to make a difference either through intellectual influence within the academic sphere, or through actual or potential use beyond the academic sphere, or both.”
1. **Research impact** &ndash; “…demonstrable economic and social impacts that have been achieved through activity within the [research institution] that builds on excellent research.”
1. **Research environment** &ndash; “…the extent to which the [research institution] has developed a research infrastructure, and a range of supporting activity, conducive to a continuing flow of excellent research and to its effective dissemination and application.”

However, the three elements are not equally weighted: “The assessment of research impact will be one of three distinct elements of the REF, being judged alongside research excellence and research environment, contributing 25% towards the overall outcome (as compared with 60% and 15% for quality and environment).” (Technopolis 2010)

A pilot exercise of the REF was conducted in 2009.

### How was ‘practical impact’ defined in the examples

The documentation published by HEFCE for the REF explained that practical impacts would include “a wide definition of impacts, including economic, social, public policy, cultural and quality of life” and that any reference to “‘impact’ or ‘social and economic impact’” implicitly included the entire wide range of impacts. (HEFCE 2009)

This did not prevent confusion about what was meant, unfortunately.  In a “lessons learned” report about the HEFCE-REF pilot exercise, one of the items given as a challenge was explaining to academic groups “that socio-economic impact was a much broader concept than economic impact” (Technopolis 2010). 

Some outside the process seem similarly unclear on the wider impact being sought by the REF.  Commenting on the high ratings received by all groups taking part in the pilot exercise, Khazragui and Hudson (2015) write, “But in part too it is a consequence of having economic impact evaluated by non-economists." The implication being that since economic impact is the dominant feature of overall impact, an over-estimation of economic impact would inevitably cause a large-magnitude effect on the results.  Therefore the high ratings can be explained by a failure to properly quantify the economic impact.

The objection of Khazragui and Hudson (2015) to a lack of quantitative methods, presumably is what led them to denigrate the nature of the evaluation process, writing, “research funders also illustrate their impact with ‘stories’.”

The authors are presumably are referring to the “narrative evidence” that was used for the REF, since “there are limitations in the extent to which the impacts of research can be ‘measured’ through quantifiable indicators.” The REF used a qualitative process.  

  “Rather than seek to **measure** the impacts in a quantifiable way, impact will be **assessed** in the REF. Expert panels will review narrative evidence supported by appropriate indicators, and produce graded impact sub-profiles for each submission; they will not seek to quantify the impacts.” (HEFCE 2009)

Quantitive data were included in the process, however.  The HEFCE directed that the case studies that were submitted by research institutions should “include a range of **indicators of impact** as supporting evidence”.  Those indicators were expected to be quantified values, such as the research income generated from other funding sources, and accountings of collaborations with companies in the private sector (HEFCE 2009).

In the Australian RFQ, Donovan (2008) reports there were competing definitions….


----

References

* Butler, L. 2008. “Using a Balanced Approach to Bibliometrics: Quantitative Performance Measures in the Australian Research Quality Framework.” *Ethics in Science and Environmental Politics* 8 (June): 83–92. doi:10.3354/esep00077.

* Department of Innovation, Industry, Science and Research. 2007. "Cancellation of Research Quality Framework implementation" [media release]. http://web.archive.org/web/20080203025537/http://minister.industry.gov.au/SenatortheHonKimCarr/Pages/CANCELLATIONOFRESEARCHQUALITYFRAMEWORKIMPLEMENTATION.aspx. Retrieved March 27, 2015.

* Donovan, Claire. 2008. “The Australian Research Quality Framework: A Live Experiment in Capturing the Social, Economic, Environmental, and Cultural Returns of Publicly Funded Research.” *New Directions for Evaluation* 2008 (118): 47–60. doi:10.1002/ev.260.

* Grant, J., P. B. Brutscheer, S. Kirk, L. Butler, and S. Wooding. 2010. "Documented Briefing: Capturing Research Impacts–a Review of International Practice". *DB-578-HEFCE*. RAND Documented Briefings, RAND Corporation. http://www.rand.org/pubs/documented_briefings/DB578.html. Retrieved November 4, 2014.

* HEFCE. 2009. *Research Excellence Framework - Second Consultation on the Assessment and Funding of Research*. HEFCE 2009/38. HEFCE. http://webarchive.nationalarchives.gov.uk/20100202100434/http://www.hefce.ac.uk/pubs/hefce/2009/09_38/09_38.pdf. Retrieved April 3, 2015.

* HEFCE. 2015. “Our Role.” Higher Education Funding Council for England. Accessed April 3. https://www.hefce.ac.uk/about/role/.

* Khazragui, Hanan, and John Hudson. 2015. “Measuring the Benefits of University Research: Impact and the REF in the UK.” *Research Evaluation* 24 (1): 51–62. doi:10.1093/reseval/rvu028.

* Technopolis Ltd. 2010. *REF Research Impact Pilot Exercise Lessons-Learned Project: Feedback on Pilot Submissions*. Higher Education Funding Council for England. http://www.ref.ac.uk/pubs/refimpactpilotlessons-learnedfeedbackonpilotsubmissions/.



